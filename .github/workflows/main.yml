name: CERN Data Analysis Pipeline (Solo An√°lisis)

# Este workflow se ejecuta cuando se hace push o manualmente
on:
  push:
    branches:
      - main
      - master
  workflow_dispatch:

jobs:
  analyze_and_upload:
    runs-on: ubuntu-latest
    
    # Define la clave para el cach√© de datos
    env:
      CACHE_KEY: cern-opendata-root-v1

    steps:
      - name: 1. Checkout Repository
        uses: actions/checkout@v4
              
      - name: 2. Set up Python Environment
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: 3. Install Python Dependencies
        run: pip install -r requirements.txt
              
      - name: 4. Restore/Download Data Cache üíæ
        uses: actions/cache@v4
        id: cache-data
        with:
          path: data
          key: ${{ runner.os }}-${{ env.CACHE_KEY }}

      - name: 5. Execute Data Analysis (Python)
        run: |
          echo "Starting analysis."
          python analysis.py
              
      - name: 6. Prepare Docs Folder (Merge Artifacts)
        # Copia los resultados generados por el an√°lisis a la carpeta 'docs'
        run: cp -r results/* docs/
                 
      - name: 7. Upload Results as Artifact ‚¨ÜÔ∏è
        # Sube la carpeta 'docs' (que ahora contiene los resultados) como un archivo zip
        uses: actions/upload-artifact@v4
        with:
          name: analysis-results-artifact
          path: docs/
